"""
Predict how benefitial is a move compared to the current state and the other posible moves
-> From -1 (not useful at all) to +1 (most defenetly useful)

For this we employ a technique similar to the value value function. We define the depth and the breadth is set to the maximal.
-> A longer Depth and a bigger Model produce better results but also require a lot of computational power.
Proposed depth = 4
Proposed model takes in sorrounding grid action to evaluate and set of actions that lead to a solution.
sorrounding grid = 5*5 with three (boxes goals walls) chanels each -> 75
set of actions 4*1 with asingle chanel each -> 4
actions that led to the solution -> 4*4 (first four action the aStarSearch does to get to the solutions)
total = 95 in
hidden = 50 per layer for 2 hidden layers
out = 1
"""


def evaluate_solution(solution):
    """
    Uses the solution length as the metric.
    Returns a higher score for a shorter solution.
    """
    return -len(solution)


def depthLimitedSearch(posPlayer, posBox, posWalls, posGoals, Logic, depth, exploredSet):
    """
    Recursively explores from the given state (posPlayer, posBox) up to a maximum depth.
    At depth zero, it obtains a solution from A* (via Logic.aStar) and returns that.
    
    Instead of simply returning the solution length, we use evaluate_solution() to compute
    a branch score.
    
    The exploredSet is a cache of states to avoid re-computation.
    """

    if depth == 0:
        if (posPlayer, posBox) in exploredSet:
            solution, ev = exploredSet[(posPlayer, posBox)]
            return exploredSet, (posPlayer, posBox), (solution, ev)
        else:
            solution = Logic.aStar(posPlayer, posBox, posWalls, posGoals, PriorityQueue, heuristic, cost)
            exploredSet[(posPlayer, posBox)] = (solution, evaluate_solution(solution))
            return exploredSet, (posPlayer, posBox), (solution, evaluate_solution(solution))
    #l = [posPlayer, posBox, posWalls, posGoals]
    #for i in l:
        #print(i)
        #print("---")
    legal_inverts, nextBoxArr = Logic.legalInverts(posPlayer, posBox, posWalls, posGoals)

    bestState, bestSolution = None, None
    bestScore = -float('inf')
    
    for idx, action in enumerate(legal_inverts):
        newPosPlayer = Logic.FastInvert(posPlayer, action[0])

        newPosBox = nextBoxArr[idx]
        # Recursively search deeper.
        exploredSet, state, bundle = depthLimitedSearch(newPosPlayer, newPosBox, posWalls, posGoals, Logic, depth - 1, exploredSet)
        currentScore = bundle[1]
        if currentScore > bestScore:
            bestScore = currentScore
            bestState, bestSolution = state, bundle[0]

    return exploredSet, bestState, (bestSolution, bestScore)

# ------------------------------
# Evaluate All Root Actions
# ------------------------------
def evaluate_root_actions(posPlayer, posBox, posWalls, posGoals, Logic, depth):
    """
    For the current state S (given by posPlayer, posBox) with legal actions,
    evaluate each action by performing a depth-limited search from the resulting state.
    Returns a list of tuples: (action, branch_score).
    """
    legal_inverts, nextBoxArr = Logic.legalInverts(posPlayer, posBox, posWalls, posGoals)
    action_scores = []
    explored_dict = {}
    for idx, action in enumerate(legal_inverts):
        newPosPlayer = Logic.FastInvert(posPlayer, action[0])

        newPosBox = nextBoxArr[idx]
        # Use an empty dictionary for caching (or pass a shared one if you wish)
        explored_dict, _, bundle = depthLimitedSearch(newPosPlayer, newPosBox, posWalls, posGoals, Logic, depth, explored_dict)
        action_scores.append((action[1], bundle[1]))
    return action_scores

def compute_target_values(action_scores):
    """
    Given a list of (action, branch_score) tuples for the current state,
    compute normalized target values for each action.
    The best action gets a target of +1, the worst gets -1, and others are scaled accordingly.
    """
    scores = [score for (_, score) in action_scores]
    max_score = max(scores)
    min_score = min(scores)
    targets = {}
    for action, score in action_scores:
        if max_score == min_score:
            target = 0.0
        else:
            target = (score - min_score) / (max_score - min_score) * 2 - 1
        targets[tuple(action)] = target
    return targets

# ------------------------------
# Feature Encoding Function
# ------------------------------
def encode_state_action(posPlayer, posBox, posWalls, posGoals, action, AstarSolution):
    """
    Encodes the state (a 5x5 window around posPlayer for boxes, walls, and goals)
    along with the action and a summary of the A* solution (one-hot encoded) into
    a fixed-length feature vector.
    
    This example simply concatenates flattened channels; adjust as needed.
    """
    # For simplicity, we assume that the channels for boxes, walls, and goals are each lists.
    boxesChannel = []
    wallsChannel = []
    goalsChannel = []
    
    for i in range(-2, 3): 
        for j in range(-2, 3):   
            current_x = posPlayer[0] + i
            current_y = posPlayer[1] + j
            currentPos = (current_x, current_y)

            boxesChannel.append(1 if currentPos in posBox else 0)
            wallsChannel.append(1 if currentPos in posWalls else 0)
            goalsChannel.append(1 if currentPos in posGoals else 0)

    # Convert to tensors.
    boxesChannel = torch.tensor(boxesChannel, dtype=torch.float32)
    wallsChannel = torch.tensor(wallsChannel, dtype=torch.float32)
    goalsChannel = torch.tensor(goalsChannel, dtype=torch.float32)
    action_tensor = torch.tensor(action, dtype=torch.float32)
    Astar_tensor = torch.tensor(AstarSolution, dtype=torch.float32)

    # Concatenate all channels into one feature vector.
    modelInput = torch.cat((
        boxesChannel.flatten(),
        wallsChannel.flatten(),
        goalsChannel.flatten(),
        action_tensor.flatten(),
        Astar_tensor.flatten()
    ))
    
    return modelInput

# ------------------------------
# TreePolicyNetwork (Model)
# ------------------------------
class TreePolicyNetwork(nn.Module): 
    def __init__(self, in_dim=95, hid_dim=50, out_dim=1, num_hidden_layers=2, lr=0.001):
        super(TreePolicyNetwork, self).__init__()
        layers = [nn.Linear(in_dim, hid_dim), nn.ReLU()]
        for _ in range(num_hidden_layers):
            layers.append(nn.Linear(hid_dim, hid_dim))
            layers.append(nn.ReLU())
        layers.append(nn.Linear(hid_dim, out_dim))
        self.model = nn.Sequential(*layers)
        self.optimizer = optim.Adam(self.parameters(), lr=lr)
        self.loss_fn = nn.MSELoss()

    def forward(self, x):
        return self.model(x)

    def train_on(self, state, difficulty):
        self.optimizer.zero_grad()
        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
        target_tensor = torch.tensor([difficulty], dtype=torch.float32).unsqueeze(0)
        output = self.forward(state_tensor)
        loss = self.loss_fn(output, target_tensor)
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def predict(self, state):
        with torch.no_grad():
            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)
            return self.forward(state_tensor).item()

# Global instance of the tree policy network for evaluation/training.
inverBFSTP = TreePolicyNetwork()



def stackFirstSteps(input_list):
    if len(input_list) < 5:
        input_list.extend([[ -1 ] * 5] * (5 - len(input_list)))

    first_five_sublists = input_list[:5]

    flattened = [item for sublist in first_five_sublists for item in sublist]
    stacked_tensor = torch.tensor(flattened).view(-1, 1)

    return stacked_tensor

# ------------------------------
# Train on a Root State
# ------------------------------
def train_root_state(posPlayer, posBox, posWalls, posGoals, Logic, model, depth=4, currentSolution=None):
    """
    For the current state S, evaluate each legal action, compute relative target values,
    and train the model on each (action, state) pair.
    
    AstarSolution_sample can be provided (e.g. the A* solution from S) to include in the feature encoding.
    """
    # Evaluate all legal actions from S.
    action_scores = evaluate_root_actions(posPlayer, posBox, posWalls, posGoals, Logic, depth)
    targets = compute_target_values(action_scores)
    
    # For each action, build the feature vector and train the model.
    for action, score in action_scores:
        target = targets[tuple(action)]
        if currentSolution == None:
            currentSolution = Logic.aStar(posPlayer, posBox, posWalls, posGoals, PriorityQueue, heuristic, cost)
        state_feature = encode_state_action(posPlayer, posBox, posWalls, posGoals, action, stackFirstSteps(currentSolution))
        print(target)
        print(state_feature)
        print(state_feature.shape)
        #loss = model.train_on(state_feature, target)
        #print(f"Trained on action {action} with target {target:.3f}, loss = {loss:.4f}")

grid = np.array([
    [1, 1, 1, 1, 1, 1, 1],
    [1, 0, 0, 0, 0, 4, 1],
    [1, 0, 0, 1, 1, 0, 1],
    [1, 0, 3, 0, 2, 0, 1],
    [1, 0, 0, 0, 0, 0, 1],
    [1, 1, 1, 1, 1, 1, 1]
])
def extract_positions(grid):
    # Walls: grid == 1
    posWalls = tuple(tuple(x) for x in np.argwhere(grid == 1))
    # Goals: grid == 4, 5, or 6
    posGoals = tuple(tuple(x) for x in np.argwhere(np.isin(grid, [4, 5, 6])))
    # Boxes: grid == 3 or 5
    posBoxes = tuple(tuple(x) for x in np.argwhere(np.isin(grid, [3, 5])))
    # Player: grid == 2 or 6 (we assume only one player)
    posPlayer_arr = np.argwhere(np.isin(grid, [2, 6]))
    if len(posPlayer_arr) == 0:
        raise ValueError("Player not found!")
    posPlayer = tuple(posPlayer_arr[0])
    return posPlayer, posBoxes, posWalls, posGoals

posPlayer, posBoxes, posWalls, posGoals = extract_positions(grid)

train_root_state(posPlayer, posBoxes, posWalls, posGoals, master(), inverBFSTP, depth=3, currentSolution=None)