Initialize library L with basic actions
Initialize neural recognition model Q
Initialize tree polocy π 

for each iteration do
    # Phase 1: Wake
    for each level in TaskSet do
        # Use neural model Q to suggest actions
        while level not completed:
            for s in leafs:
                π(s) = Prob
            Prune leafs given Probs
            old_leafs = leafs
            leafs = []
            for s in old_leafs:
                Q(s) = Probs accros legal actions in L 
        
        # Store the solution
        if solution_found:
            Solutions[lvl] = solution
    
    # Phase 2: Dream
    # Refine the library by compressing successful programs
    new_macroAction = Compress(Solutions, L)
    L.extend(new_macroAction)
    
    # Phase 3: Train Recognition Model and Tree Policy
    TrainRecognitionModel(Q, Solutions, L)
    TrinTreePolicy(π, solutions, Q)

    # Check for convergence or stopping criteria
    if Converged(Solutions, L):
        break
end for

return L, Q, π, Solutions


qsolve:
def DepthAndBreadthLimitedSearch(start_player, start_box, max_depth, max_breadth, n, m, Q_network):
    """
    Forward search using a neural network heuristic (Q function).
    When breadth exceeds max_breadth, we split the frontier into batches of size n, 
    evaluate them using Q, and perform probabilistic selection to retain m nodes per batch.
    """

    # Initialize the search with the root node
    root = Node(state=(start_player, start_box))
    frontier = Frontier()
    frontier.push(root)

    depth = max_depth

    while depth > 0 and not frontier.is_empty():
        if frontier.size() < max_breadth:
            # **Full Expansion: Explore all children without pruning**
            new_nodes = []

            while not frontier.is_empty():
                current_node = frontier.pop()
                current_player, current_box = current_node.state

                # Get all possible moves and resulting states
                legal_moves, next_states = get_legal_moves(current_player, current_box)

                for i in range(len(legal_moves)):
                    new_player, new_box = next_states[i]
                    new_state = (new_player, tuple(sorted(new_box)))

                    # Compute heuristic with Q-network if not cached
                    if new_state not in solution_cache:
                        solution = run_aStar(new_player, new_box)
                        box_lines = compute_box_lines(solution, new_player, new_box)
                        store_in_cache(new_state, solution, len(solution), box_lines)

                    # Create new node with parent tracking
                    new_node = Node(state=new_state, parent=current_node, action=legal_moves[i])
                    new_nodes.append(new_node)

            # Push all new nodes into the frontier
            for node in new_nodes:
                frontier.push(node)

            depth -= 1
        
        else:
            # **Breadth Exceeds Limit: Use Q-Network for Pruning**
            nodes_list = list(frontier.frontier)  # Convert deque to list
            batches = [nodes_list[i:i + n] for i in range(0, len(nodes_list), n)]  # Split into size-n batches
            
            selected_nodes = []

            for batch in batches:
                states_batch = [node.state for node in batch]  # Extract states
                q_values = Q_network(states_batch)  # Evaluate using the Q-network
                
                # Convert Q-values to a probability distribution (lower Q-value = better state)
                probabilities = softmax(-q_values)  # Use softmax over negative Q-values
                
                # Select m nodes based on probability
                selected_indices = random.choices(range(len(batch)), weights=probabilities, k=m)
                selected_nodes.extend([batch[i] for i in selected_indices])

            # Clear the frontier and push only selected nodes
            frontier.frontier.clear()
            for node in selected_nodes:
                frontier.push(node)

            depth -= 1
